config_version: config_5
trainable_weights: lora
convo_format: paired_next_stage
model_fpath: baichuan-inc/Baichuan-7B
learning_rate: 6.0e-7
load_in_8bit: True
load_in_fp16: False
fp16_training: True
deepspeed: False
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
model_max_length: 512
use_gradient_checkpointing: False
target_modules: ['W_pack']